{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26db0d3d-ae66-4d95-b99c-53a8ae90bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-01 12:03:14\u001b[0m - \u001b[34mHive\u001b[0m - \u001b[1;30mINFO\u001b[0m - Hive GPU Available: True\n",
      "usage: nnunet_prepare_data_folder [-h] -i INPUT_DATA_FOLDER\n",
      "                                  [--task-ID TASK_ID] --task-name TASK_NAME\n",
      "                                  [--test-split [0-100]] --config-file\n",
      "                                  CONFIG_FILE [-v | -q]\n",
      "\n",
      "Prepare Dataset folder according to the nnUNet specifications, creating and populating the subfolders ``imagesTr``,\n",
      "``labelsTr``, ``imagesTs`` and ``labelsTs``. In addition, a JSON instance configuration file (as required by nnUNet)\n",
      "for each label mask is generated, alongside a summary of the train/test split of the dataset.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i INPUT_DATA_FOLDER, --input-data-folder INPUT_DATA_FOLDER\n",
      "                        Input Dataset folder\n",
      "  --task-ID TASK_ID     Task ID used in the folder path tree creation (Default: 100)\n",
      "  --task-name TASK_NAME\n",
      "                        Task Name used in the folder path tree creation.\n",
      "  --test-split [0-100]  Split value ( in % ) to create Test set from Dataset (Default: 20)\n",
      "  --config-file CONFIG_FILE\n",
      "                        Configuration JSON file with experiment and dataset parameters.\n",
      "  -v, --verbose         Enable verbose output in terminal. Add multiple times to increase verbosity.\n",
      "  -q, --silent          Suppress most log outputs in terminal.\n",
      "\n",
      "Example call:\n",
      "::\n",
      "    nnunet_prepare_data_folder.py -i /PATH/TO/DATA_FOLDER --task-ID 000 --task-name Example --config-file Example_config.json\n",
      "    nnunet_prepare_data_folder.py -i /PATH/TO/DATA_FOLDER --task-ID 000 --task-name Example --config-file Example_config.json --test-split 30\n"
     ]
    }
   ],
   "source": [
    "!nnunet_prepare_data_folder -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83da15d1-4bff-45dc-8435-87c095bb7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-01 12:03:21\u001b[0m - \u001b[34mHive\u001b[0m - \u001b[1;30mINFO\u001b[0m - Hive GPU Available: True\n",
      "usage: nnunet_run_preprocessing [-h] --config-file CONFIG_FILE [-v | -q]\n",
      "\n",
      "Run nnUNet command to extract dataset fingerprints, create experiment plan, preprocessing and ( optionally ) verify \n",
      "the dataset integrity.\n",
      "The CL script called is  ``nnUNetv2_plan_and_preprocess``, with the arguments extracted from the given configuration file.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --config-file CONFIG_FILE\n",
      "                        File path for the configuration dictionary, used to retrieve experiments variables (Task_ID) \n",
      "  -v, --verbose         Enable verbose output in terminal. Add multiple times to increase verbosity.\n",
      "  -q, --silent          Suppress most log outputs in terminal.\n",
      "\n",
      "Example call:\n",
      "::\n",
      "    nnunet_run_preprocessing.py --config-file /PATH/TO/CONFIG_FILE.json\n"
     ]
    }
   ],
   "source": [
    "!nnunet_run_preprocessing -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d978d7c5-e243-4119-8248-6dd599d565dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#python -m monai.bundle init_bundle nnUNet\n",
    "\n",
    "mkdir nnUNet/bundle_scripts\n",
    "touch nnUNet/bundle_scripts/__init__.py\n",
    "which tree && tree nnUNet || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ffa665d-f3ad-46dd-9712-53f452481dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.22.2\n",
      "Pytorch version: 2.1.0a0+fe05266\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: /home/<username>/.local/lib/python3.8/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.0\n",
      "scikit-image version: 0.21.0\n",
      "scipy version: 1.10.1\n",
      "Pillow version: 9.2.0\n",
      "Tensorboard version: 2.9.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.0a0\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.4\n",
      "pandas version: 1.5.2\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.9.2\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "\n",
    "monai.config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a61a28aa-4f9f-43e4-aa5e-677079e9d388",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nnunetv2\n",
      "Version: 2.1.1\n",
      "Summary: nnU-Net. Framework for out-of-the box biomedical image segmentation.\n",
      "Home-page: https://github.com/MIC-DKFZ/nnUNet\n",
      "Author: Helmholtz Imaging Applied Computer Vision Lab, Division of Medical Image Computing, German Cancer Research Center\n",
      "Author-email: f.isensee@dkfz-heidelberg.de\n",
      "License: Apache License Version 2.0, January 2004\n",
      "Location: /opt/code/nnunet\n",
      "Requires: torch, acvl-utils, dynamic-network-architectures, tqdm, dicom2nifti, scikit-image, scipy, batchgenerators, numpy, scikit-learn, scikit-image, SimpleITK, pandas, graphviz, tifffile, requests, nibabel, matplotlib, seaborn, imagecodecs, yacs, mlflow\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show nnunetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9ce18b2-d267-4688-b351-a2bcbfdbebc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nnUNet/configs/metadata.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNet/configs/metadata.json\n",
    "\n",
    "{\n",
    "    \"version\": \"0.1.0\",\n",
    "    \"monai_version\": \"1.3.0\",\n",
    "    \"pytorch_version\": \"2.1.0\",\n",
    "    \"numpy_version\": \"1.22.2\",\n",
    "    \"optional_packages_version\": {\n",
    "        \"nnunet\": \"2.1.1\"\n",
    "        \n",
    "        },\n",
    "    \"task\": \"nnUNet Bundle\",\n",
    "    \"description\": \"A nnUNet MONAI Bundle, used to run nnUNet Experiments within the MONAI Bundle framework.\",\n",
    "    \"authors\": \"Simone Bendazzoli\",\n",
    "    \"data_type\": \"nifti\",\n",
    "    \"references\": [\n",
    "        \"Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\"\n",
    "    ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23b52965-f9b2-4fff-aa57-8ab1469bcc70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nnUNet/configs/prepare.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNet/configs/prepare.yaml\n",
    "\n",
    "imports:\n",
    "- $import subprocess\n",
    "\n",
    "data_dir: .\n",
    "task_id: \"\"\n",
    "hive_config_file: \"\"\n",
    "task_name: \"\"\n",
    "\n",
    "cmd:\n",
    "- \"nnunet_prepare_data_folder\"\n",
    "- \"-i\"\n",
    "- '@data_dir'\n",
    "- \"--task-ID\"\n",
    "- '$str(@task_id)'\n",
    "- \"--task-name\"\n",
    "- '@task_name'\n",
    "- \"--test-split\"\n",
    "- \"0\"\n",
    "- \"--config-file\"\n",
    "- '@hive_config_file'\n",
    "\n",
    "prepare:\n",
    "- '$print(@cmd)'\n",
    "- '$subprocess.run(@cmd)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f643a7c7-f252-4fce-b8bf-a7f1dbdfadd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nnUNet/configs/preprocess.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNet/configs/preprocess.yaml\n",
    "\n",
    "imports:\n",
    "- $import subprocess\n",
    "- $from pathlib import Path\n",
    "- $import os\n",
    "- $import json\n",
    "\n",
    "hive_config_file: \"\"\n",
    "config_dict: \"$json.load(open(@hive_config_file))\"\n",
    "task_id: \"\"\n",
    "\n",
    "cmd:\n",
    "- \"nnunet_run_preprocessing\"\n",
    "- \"--config-file\"\n",
    "- \"$str(Path(os.environ['ROOT_FOLDER']).joinpath(@config_dict['Experiment Name'], @config_dict['Experiment Name'] + '_results','Dataset' + str(@task_id) + '_' + @config_dict['Experiment Name'] + '.json'))\"\n",
    "\n",
    "preprocess:\n",
    "- '$print(@cmd)'\n",
    "- '$subprocess.run(@cmd)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8225bb1-7f82-4775-a373-70207bc31656",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nnUNet/bundle_scripts/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNet/bundle_scripts/trainer.py\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from typing import Union, Optional\n",
    "from torch.backends import cudnn\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def get_nnunet_trainer(dataset_name_or_id: Union[str, int],\n",
    "             configuration: str, fold: Union[int, str],\n",
    "             hive_config_file: str,                        #To set env variables\n",
    "             trainer_class_name: str = 'nnUNetTrainer',\n",
    "             plans_identifier: str = 'nnUNetPlans',\n",
    "             pretrained_weights: Optional[str] = None,\n",
    "             num_gpus: int = 1,\n",
    "             use_compressed_data: bool = False,\n",
    "             export_validation_probabilities: bool = False,\n",
    "             continue_training: bool = False,\n",
    "             only_run_validation: bool = False,\n",
    "             disable_checkpointing: bool = False,\n",
    "             val_with_best: bool = False,\n",
    "             device: torch.device = torch.device('cuda')): # From nnUNet/nnunetv2/run/run_training.py#run_training\n",
    "    \n",
    "    ## Block Added\n",
    "    with open(hive_config_file,\"r\") as f:\n",
    "        hive_config_dict = json.load(f)\n",
    "    \n",
    "    os.environ[\"nnUNet_raw\"] = str(Path(hive_config_dict[\"base_folder\"]).joinpath(\"nnUNet_raw_data\"))\n",
    "    os.environ[\"nnUNet_preprocessed\"] = hive_config_dict[\"preprocessing_folder\"]\n",
    "    os.environ[\"nnUNet_results\"] = hive_config_dict[\"results_folder\"]\n",
    "\n",
    "    from nnunetv2.run.run_training import get_trainer_from_args, maybe_load_checkpoint\n",
    "    ##\n",
    "    \n",
    "    if isinstance(fold, str):\n",
    "        if fold != 'all':\n",
    "            try:\n",
    "                fold = int(fold)\n",
    "            except ValueError as e:\n",
    "                print(f'Unable to convert given value for fold to int: {fold}. fold must bei either \"all\" or an integer!')\n",
    "                raise e\n",
    "\n",
    "   \n",
    "\n",
    "    if int(num_gpus) > 1:\n",
    "        ... # Disable for now\n",
    "    else:\n",
    "        nnunet_trainer = get_trainer_from_args(str(dataset_name_or_id), configuration, fold, trainer_class_name,\n",
    "                                               plans_identifier, use_compressed_data, device=device)\n",
    "\n",
    "        if disable_checkpointing:\n",
    "            nnunet_trainer.disable_checkpointing = disable_checkpointing\n",
    "\n",
    "        assert not (continue_training and only_run_validation), f'Cannot set --c and --val flag at the same time. Dummy.'\n",
    "\n",
    "        maybe_load_checkpoint(nnunet_trainer, continue_training, only_run_validation, pretrained_weights)\n",
    "        nnunet_trainer.on_train_start() # To Initialize Trainer\n",
    "        if torch.cuda.is_available():\n",
    "            cudnn.deterministic = False\n",
    "            cudnn.benchmark = True\n",
    "\n",
    "        # Skip Training and Validation Phase\n",
    "\n",
    "        return nnunet_trainer\n",
    "    \n",
    "def prepare_nnunet_batch(batch,device,non_blocking):\n",
    "    data = batch[\"data\"].to(device, non_blocking=non_blocking)\n",
    "    if isinstance(batch[\"target\"], list):\n",
    "        target = [i.to(device, non_blocking=non_blocking) for i in batch[\"target\"]]\n",
    "    else:\n",
    "        target = batch[\"target\"].to(device, non_blocking=non_blocking)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "892cd206-138f-4abc-b2b7-1be501226423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nnUNet/configs/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNet/configs/train.yaml\n",
    "\n",
    "imports:\n",
    "- $from bundle_scripts.trainer import get_nnunet_trainer\n",
    "- $from bundle_scripts.trainer import prepare_nnunet_batch\n",
    "- $import json\n",
    "- $from pathlib import Path\n",
    "- $import os\n",
    "- $from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "- $import shutil\n",
    "\n",
    "task_id: \"\"\n",
    "bundle_root: \"\"\n",
    "\n",
    "hive_config_file: \"\"\n",
    "hive_config_dict: \"$json.load(open(@hive_config_file))\"\n",
    "hive_config_file_final: \"$str(Path(os.environ['ROOT_FOLDER']).joinpath(@hive_config_dict['Experiment Name'], @hive_config_dict['Experiment Name'] + '_results','Dataset' + str(@task_id) + '_' + @hive_config_dict['Experiment Name'] + '.json'))\"\n",
    "nnunet_model_folder: \"$str(Path(os.environ['ROOT_FOLDER']).joinpath(@hive_config_dict['Experiment Name'], @hive_config_dict['Experiment Name'] + '_results','Dataset' + str(@task_id) + '_' + @hive_config_dict['Experiment Name'],'nnUNetTrainerHive__nnUNetPlans__3d_fullres'))\"\n",
    "\n",
    "nnunet_config:\n",
    "    device: \"$torch.device('cuda')\"\n",
    "    dataset_name_or_id: \"@task_id\"\n",
    "    configuration: \"3d_fullres\"\n",
    "    fold: 0\n",
    "    trainer_class_name: \"nnUNetTrainerHive\"\n",
    "    num_gpus: \"1\"\n",
    "    hive_config_file: \"@hive_config_file_final\"\n",
    "\n",
    "\n",
    "nnunet_trainer: \"$get_nnunet_trainer(**@nnunet_config)\"\n",
    "\n",
    "\n",
    "# Needed at Inference Time. See inference.yaml\n",
    "checkpoint:\n",
    "    init_args: '$@nnunet_trainer.my_init_kwargs'\n",
    "    trainer_name: '$@nnunet_trainer.__class__.__name__'\n",
    "    inference_allowed_mirroring_axes: '$@nnunet_trainer.inference_allowed_mirroring_axes'\n",
    "\n",
    "checkpoint_filename: \"$@bundle_root+'/models/nnunet_checkpoint.pth'\"\n",
    "\n",
    "postprocessing:\n",
    "  _target_: \"Compose\"\n",
    "  transforms:\n",
    "  - _target_: Lambdad\n",
    "    keys:\n",
    "    - \"pred\"\n",
    "    - \"label\"\n",
    "    func: \"$lambda x: x[0]\"\n",
    "  - _target_: Activationsd\n",
    "    keys:\n",
    "    - \"pred\"\n",
    "    softmax: True\n",
    "\n",
    "    \n",
    "validation:\n",
    "    dataloader: \"$@nnunet_trainer.dataloader_val\"\n",
    "    pbar:\n",
    "      _target_: \"ignite.contrib.handlers.tqdm_logger.ProgressBar\"\n",
    "    key_metric:\n",
    "        Val_Dice:\n",
    "            _target_: \"MeanDice\"\n",
    "            output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "            reduction: \"mean\"\n",
    "    additional_metrics:\n",
    "        Val_Dice_per_class:\n",
    "            _target_: \"MeanDice\"\n",
    "            output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "            reduction: \"mean_batch\"\n",
    "      \n",
    "train:\n",
    "    dataloader: \"$@nnunet_trainer.dataloader_train\"\n",
    "    pbar:\n",
    "      _target_: \"ignite.contrib.handlers.tqdm_logger.ProgressBar\"\n",
    "    key_metric:\n",
    "        Train_Dice:\n",
    "            _target_: \"MeanDice\"\n",
    "            output_transform: \"$monai.handlers.from_engine(['pred', 'label'])\"\n",
    "            reduction: \"mean_batch\"\n",
    "    handlers:\n",
    "    - _target_: \"LrScheduleHandler\"\n",
    "      lr_scheduler: \"$@nnunet_trainer.lr_scheduler\"\n",
    "    - _target_: \"ValidationHandler\"\n",
    "      interval: 1\n",
    "      validator: \"$@validator\"\n",
    "      exec_at_start: True\n",
    "    - _target_: \"CheckpointSaver\"\n",
    "      save_dir: \"$@bundle_root+'/models'\"\n",
    "      save_interval: 1\n",
    "      n_saved: 1\n",
    "      save_dict:\n",
    "        network_weights: '$@nnunet_trainer.network'\n",
    "        optimizer_state: '$@nnunet_trainer.optimizer'\n",
    "        scheduler: '$@nnunet_trainer.lr_scheduler'\n",
    "\n",
    "validator:\n",
    "    _target_: \"SupervisedEvaluator\"\n",
    "    postprocessing: \"$@postprocessing\"\n",
    "    device: \"$@nnunet_trainer.device\"\n",
    "    val_data_loader: \"$@validation#dataloader\"\n",
    "    network: \"$@nnunet_trainer.network\"\n",
    "    key_val_metric: \"@validation#key_metric\"\n",
    "    additional_metrics: \"$@validation#additional_metrics\"\n",
    "    epoch_length: \"$int(@nnunet_trainer.num_val_iterations_per_epoch)\"\n",
    "    prepare_batch: \"$prepare_nnunet_batch\"\n",
    "\n",
    "trainer:\n",
    "    _target_: \"SupervisedTrainer\"\n",
    "    postprocessing: \"$@postprocessing\"\n",
    "    device: \"$@nnunet_trainer.device\"\n",
    "    max_epochs: \"$int(@nnunet_trainer.num_epochs)\"\n",
    "    train_data_loader: \"$@train#dataloader\"\n",
    "    network: \"$@nnunet_trainer.network\"\n",
    "    key_train_metric: \"@train#key_metric\"    \n",
    "    optimizer: \"$@nnunet_trainer.optimizer\"\n",
    "    loss_function: \"$@nnunet_trainer.loss\"\n",
    "    epoch_length: \"$int(@nnunet_trainer.num_iterations_per_epoch)\"\n",
    "    prepare_batch: \"$prepare_nnunet_batch\"\n",
    "    train_handlers: \"$@train#handlers\"\n",
    "    \n",
    "run:\n",
    "- \"$@train#pbar.attach(@trainer,output_transform=lambda x: {'loss': x[0]['loss']})\"\n",
    "#- \"$@validation#pbar.attach(@validator,metric_names=['Val_Dice'])\"\n",
    "- \"$torch.save(@checkpoint,@checkpoint_filename)\"\n",
    "- \"$shutil.copy(Path(@nnunet_model_folder).joinpath('dataset.json'), @bundle_root+'/models/dataset.json')\"\n",
    "- \"$shutil.copy(Path(@nnunet_model_folder).joinpath('plans.json'), @bundle_root+'/models/plans.json')\"\n",
    "- \"$@trainer.run()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17960054-f508-4c0e-9f81-1c67327ef5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/__init__.py\", line 58, in <module>\n",
      "    load_submodules(sys.modules[__name__], False, exclude_pattern=excludes)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/utils/module.py\", line 211, in load_submodules\n",
      "    mod = import_module(name)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/apps/__init__.py\", line 14, in <module>\n",
      "    from .datasets import CrossValidation, DecathlonDataset, MedNISTDataset, TciaDataset\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/apps/datasets.py\", line 32, in <module>\n",
      "    from monai.data import (\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/data/__init__.py\", line 29, in <module>\n",
      "    from .dataset import (\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/data/dataset.py\", line 39, in <module>\n",
      "    from monai.transforms import (\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/transforms/__init__.py\", line 93, in <module>\n",
      "    from .intensity.array import (\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/transforms/intensity/array.py\", line 29, in <module>\n",
      "    from monai.data.ultrasound_confidence_map import UltrasoundConfidenceMap\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/data/ultrasound_confidence_map.py\", line 21, in <module>\n",
      "    cv2, _ = optional_import(\"cv2\")\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/utils/module.py\", line 396, in optional_import\n",
      "    pkg = __import__(module)  # top level module\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/cv2/__init__.py\", line 181, in <module>\n",
      "    bootstrap()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/cv2/__init__.py\", line 153, in bootstrap\n",
      "    native_module = importlib.import_module(\"cv2\")\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=2833): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./nnUNet\"\n",
    "\n",
    "export PYTHONPATH=$BUNDLE\n",
    "export ROOT_FOLDER=\"/home/jovyan/Experiments\"\n",
    "\n",
    "python -m monai.bundle run prepare \\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --data_dir \"/mnt/cifs/COVID-19_dataset\" \\\n",
    "    --task_id \"601\" \\\n",
    "    --hive_config_file \"/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json\" \\\n",
    "    --task_name \"LungLobeSeg_Demo_nnUNet_3D_fullres\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --config_file \"$BUNDLE/configs/prepare.yaml\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "863026e3-918e-4703-9a30-268dd15ba6c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01 12:41:30,054 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2024-02-01 12:41:30,054 - INFO - > config_file: './nnUNet/configs/preprocess.yaml'\n",
      "2024-02-01 12:41:30,054 - INFO - > meta_file: './nnUNet/configs/metadata.json'\n",
      "2024-02-01 12:41:30,054 - INFO - > run_id: 'preprocess'\n",
      "2024-02-01 12:41:30,054 - INFO - > bundle_root: './nnUNet'\n",
      "2024-02-01 12:41:30,054 - INFO - > data_dir: '/mnt/cifs/COVID-19_dataset'\n",
      "2024-02-01 12:41:30,055 - INFO - > task_id: 601\n",
      "2024-02-01 12:41:30,055 - INFO - > hive_config_file: '/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json'\n",
      "2024-02-01 12:41:30,055 - INFO - > task_name: 'LungLobeSeg_Demo_nnUNet_3D_fullres'\n",
      "2024-02-01 12:41:30,055 - INFO - ---\n",
      "\n",
      "\n",
      "2024-02-01 12:41:30,055 - WARNING - Default logging file in nnUNet/configs/logging.conf does not exist, skipping logging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 12:41:30 - Hive - INFO - Hive GPU Available: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint extraction...\n",
      "Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "Experiment planning...\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.03       0.71918958 0.71918958]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [291.26213592 497.08737864 497.08737864]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.0609     0.74076526 0.74076526]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [282.77877274 482.60910548 482.60910548]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.092727   0.76298822 0.76298822]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [274.54249781 468.55252959 468.55252959]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.12550881 0.78587787 0.78587787]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [266.54611437 454.90536853 454.90536853]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.15927407 0.8094542  0.8094542 ]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [258.78263532 441.6556976  441.6556976 ]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.1940523  0.83373783 0.83373783]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [251.24527701 428.79193942 428.79193942]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.22987387 0.85874996 0.85874996]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [243.9274534  416.30285381 416.30285381]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.26677008 0.88451246 0.88451246]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [236.82277029 404.17752797 404.17752797]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.30477318 0.91104784 0.91104784]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [229.9250197  392.40536696 392.40536696]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.34391638 0.93837927 0.93837927]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [223.22817447 380.97608443 380.97608443]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.38423387 0.96653065 0.96653065]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [216.72638298 369.87969362 369.87969362]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.42576089 0.99552657 0.99552657]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [210.41396406 359.10649866 359.10649866]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.46853371 1.02539237 1.02539237]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [204.285402   348.64708608 348.64708608]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.51258972 1.05615414 1.05615414]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [198.33534175 338.49231658 338.49231658]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.55796742 1.08783876 1.08783876]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [192.55858422 328.63331707 328.63331707]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.60470644 1.12047393 1.12047393]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [186.95008177 319.06147288 319.06147288]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.65284763 1.15408814 1.15408814]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [181.50493375 309.76842027 309.76842027]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.70243306 1.18871079 1.18871079]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [176.21838228 300.7460391  300.7460391 ]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.75350605 1.22437211 1.22437211]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [171.08580804 291.98644573 291.98644573]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.80611123 1.26110327 1.26110327]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [166.10272626 283.48198614 283.48198614]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.86029457 1.29893637 1.29893637]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [161.26478277 275.22522927 275.22522927]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.91610341 1.33790446 1.33790446]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [156.56775026 267.20896045 267.20896045]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.97358651 1.3780416  1.3780416 ]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [152.00752453 259.4261752  259.4261752 ]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [2.03279411 1.41938285 1.41938285]. \n",
      "Current patch size: [ 96 160 160]. \n",
      "Current median shape: [147.5801209  251.87007301 251.87007301]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': array([512, 512]), 'median_image_size_in_voxels': array([512., 512.]), 'spacing': array([0.69824231, 0.69824231]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': (2, 2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2, 2), 'num_pool_per_axis': [7, 7], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D lowres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': array([ 96, 160, 160]), 'median_image_size_in_voxels': [148, 252, 252], 'spacing': array([2.03279411, 1.41938285, 1.41938285]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'}\n",
      "\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': array([ 96, 160, 160]), 'median_image_size_in_voxels': array([300., 512., 512.]), 'spacing': array([1.        , 0.69824231, 0.69824231]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True}\n",
      "\n",
      "Plans were saved to /home/jovyan/Experiments/LungLobeSeg_Demo_nnUNet_3D_fullres/LungLobeSeg_Demo_nnUNet_3D_fullres_preprocess/Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres\n",
      "Configuration: 3d_fullres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.27it/s]\n",
      "100%|██████████| 10/10 [03:12<00:00, 19.25s/it]\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.bundle.workflows ConfigWorkflow.__init__:workflow_type: Current default value of argument `workflow_type=None` has been deprecated since version 1.2. It will be changed to `workflow_type=train` in version 1.4.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nnunet_run_preprocessing', '--config-file', '/home/jovyan/Experiments/LungLobeSeg_Demo_nnUNet_3D_fullres/LungLobeSeg_Demo_nnUNet_3D_fullres_results/Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres.json']\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./nnUNet\"\n",
    "\n",
    "export PYTHONPATH=$BUNDLE\n",
    "export N_THREADS=1\n",
    "export ROOT_FOLDER=\"/home/jovyan/Experiments\"\n",
    "\n",
    "python -m monai.bundle run preprocess \\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --data_dir \"/mnt/cifs/COVID-19_dataset\" \\\n",
    "    --task_id \"601\" \\\n",
    "    --hive_config_file \"/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json\" \\\n",
    "    --task_name \"LungLobeSeg_Demo_nnUNet_3D_fullres\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --config_file \"$BUNDLE/configs/preprocess.yaml\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80b20699-e5b9-49f7-9f90-074db0c563c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01 13:36:30,821 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2024-02-01 13:36:30,821 - INFO - > config_file: './nnUNet/configs/train.yaml'\n",
      "2024-02-01 13:36:30,821 - INFO - > meta_file: './nnUNet/configs/metadata.json'\n",
      "2024-02-01 13:36:30,821 - INFO - > logging_file: './nnUNet/configs/logging.conf'\n",
      "2024-02-01 13:36:30,821 - INFO - > bundle_root: './nnUNet'\n",
      "2024-02-01 13:36:30,821 - INFO - > task_id: 601\n",
      "2024-02-01 13:36:30,821 - INFO - > hive_config_file: '/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json'\n",
      "2024-02-01 13:36:30,821 - INFO - ---\n",
      "\n",
      "\n",
      "2024-02-01 13:36:30,822 - INFO - Setting logging properties based on config: ./nnUNet/configs/logging.conf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.bundle.workflows ConfigWorkflow.__init__:workflow_type: Current default value of argument `workflow_type=None` has been deprecated since version 1.2. It will be changed to `workflow_type=train` in version 1.4.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 32, 32], 'median_image_size_in_voxels': [300.0, 512.0, 512.0], 'spacing': [1.0, 0.6982423067092896, 0.6982423067092896], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 0.6982423067092896, 0.6982423067092896], 'original_median_shape_after_transp': [263, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2125.0, 'mean': -728.0990600585938, 'median': -821.0, 'min': -1023.0, 'percentile_00_5': -1023.0, 'percentile_99_5': 136.0, 'std': 253.64418029785156}}} \n",
      "\n",
      "2024-02-01 13:36:31.939290: unpacking dataset...\n",
      "2024-02-01 13:36:32.235946: unpacking done...\n",
      "2024-02-01 13:36:32.236570: do_dummy_2d_data_aug: False\n",
      "2024-02-01 13:36:32.236914: Using splits from existing split file: /home/jovyan/Experiments/LungLobeSeg_Demo_nnUNet_3D_fullres/LungLobeSeg_Demo_nnUNet_3D_fullres_preprocess/Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres/splits_final.json\n",
      "2024-02-01 13:36:32.237036: The split file contains 5 splits.\n",
      "2024-02-01 13:36:32.237075: Desired fold for training: 0\n",
      "2024-02-01 13:36:32.237109: This split has 8 training and 2 validation cases.\n",
      "2024-02-01 13:36:32.591369: Unable to plot network architecture:\n",
      "2024-02-01 13:36:32.591697: module 'torch.onnx' has no attribute '_optimize_trace'\n",
      "2024-02-01 13:36:32,637 - ignite.engine.engine.SupervisedTrainer - INFO - Engine run resuming from iteration 0, epoch 0 until 1000 epochs\n",
      "2024-02-01 13:36:32,637 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch -1 until 1 epochs\n",
      "using pin_memory on device 0\n",
      "2024-02-01 13:36:36,209 - ignite.engine.engine.SupervisedEvaluator - INFO - Got new best metric of Val_Dice: 0.7251249551773071\n",
      "2024-02-01 13:36:36,209 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[0] Complete. Time taken: 00:00:03.572\n",
      "2024-02-01 13:36:36,209 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:03.572\n",
      "using pin_memory on device 0\n",
      "2024-02-01 13:36:52,717 - ignite.engine.engine.SupervisedTrainer - INFO - Current learning rate: 0.009990999549834914\n",
      "2024-02-01 13:36:52,717 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2024-02-01 13:36:54,035 - ignite.engine.engine.SupervisedEvaluator - INFO - Got new best metric of Val_Dice: 0.7752974629402161\n",
      "2024-02-01 13:36:54,035 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:01.318\n",
      "2024-02-01 13:36:54,035 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:01.318\n",
      "2024-02-01 13:36:54,273 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]: [249/250] 100%|█████████▉, loss=1.02 [00:13<00:00]/home/jovyan/.local/lib/python3.8/site-packages/monai/engines/workflow.py:250: UserWarning: Key metric is not a scalar value, skip the metric comparison with the current best metric.Please set other metrics as the key metric, or change the `reduction` mode to 'mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01 13:36:54,273 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[1] Complete. Time taken: 00:00:18.064\n",
      "2024-02-01 13:37:07,915 - ignite.engine.engine.SupervisedTrainer - INFO - Current learning rate: 0.009981998198678612\n",
      "2024-02-01 13:37:07,915 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 1 until 2 epochs\n",
      "2024-02-01 13:37:09,247 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[2] Complete. Time taken: 00:00:01.332\n",
      "2024-02-01 13:37:09,247 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:01.332\n",
      "2024-02-01 13:37:09,523 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 2\n",
      "2024-02-01 13:37:09,524 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[2] Complete. Time taken: 00:00:15.251\n",
      "2024-02-01 13:37:23,262 - ignite.engine.engine.SupervisedTrainer - INFO - Current learning rate: 0.009972995945537971\n",
      "2024-02-01 13:37:23,262 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 2 until 3 epochs\n",
      "2024-02-01 13:37:24,629 - ignite.engine.engine.SupervisedEvaluator - INFO - Got new best metric of Val_Dice: 0.7792598605155945\n",
      "2024-02-01 13:37:24,630 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[3] Complete. Time taken: 00:00:01.368\n",
      "2024-02-01 13:37:24,630 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:01.368\n",
      "2024-02-01 13:37:24,867 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 3\n",
      "2024-02-01 13:37:24,867 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[3] Complete. Time taken: 00:00:15.343\n",
      "2024-02-01 13:37:38,668 - ignite.engine.engine.SupervisedTrainer - INFO - Current learning rate: 0.00996399278941777\n",
      "2024-02-01 13:37:38,668 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 3 until 4 epochs\n",
      "2024-02-01 13:37:40,001 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[4] Complete. Time taken: 00:00:01.333\n",
      "2024-02-01 13:37:40,001 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:01.333\n",
      "2024-02-01 13:37:40,225 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 4\n",
      "2024-02-01 13:37:40,225 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[4] Complete. Time taken: 00:00:15.359\n",
      "2024-02-01 13:37:54,015 - ignite.engine.engine.SupervisedTrainer - INFO - Current learning rate: 0.00995498872932069\n",
      "2024-02-01 13:37:54,016 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run resuming from iteration 0, epoch 4 until 5 epochs\n",
      "2024-02-01 13:37:55,353 - ignite.engine.engine.SupervisedEvaluator - INFO - Epoch[5] Complete. Time taken: 00:00:01.337\n",
      "2024-02-01 13:37:55,353 - ignite.engine.engine.SupervisedEvaluator - INFO - Engine run complete. Time taken: 00:00:01.337\n",
      "2024-02-01 13:37:55,579 - ignite.engine.engine.SupervisedTrainer - INFO - Saved checkpoint at epoch: 5\n",
      "2024-02-01 13:37:55,579 - ignite.engine.engine.SupervisedTrainer - INFO - Epoch[5] Complete. Time taken: 00:00:15.353\n",
      "2024-02-01 13:38:02,386 - ignite.engine.engine.SupervisedTrainer - ERROR - Engine run is terminating due to exception: \n",
      "Error while terminating subprocess (pid=4510): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./nnUNet\"\n",
    "\n",
    "export PYTHONPATH=$BUNDLE\n",
    "export ROOT_FOLDER=\"/home/jovyan/Experiments\"\n",
    "\n",
    "python -m monai.bundle run\\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --task_id \"601\" \\\n",
    "    --hive_config_file \"/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --logging_file \"$BUNDLE/configs/logging.conf\" \\\n",
    "    --config_file \"$BUNDLE/configs/train.yaml\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8eefd2-6003-430a-8a7c-183b79beb71c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nnUNetTest/bundle_scripts/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNetTest/bundle_scripts/inference.py\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "\n",
    "\n",
    "from batchgenerators.utilities.file_and_folder_operations import load_json, join, isfile, maybe_mkdir_p, isdir, subdirs, save_json\n",
    "\n",
    "def initialize_from_trained_model_folder( predictor,\n",
    "                                          model_training_output_dir: str,\n",
    "                                             use_folds,\n",
    "                                             nnunet_checkpoint_name: str = 'nnunet_checkpoint.pth', monai_checkpoint_name: str = 'model.pt'):\n",
    "    \n",
    "        from nnunetv2.utilities.plans_handling.plans_handler import PlansManager, ConfigurationManager\n",
    "        from nnunetv2.utilities.label_handling.label_handling import determine_num_input_channels\n",
    "        from nnunetv2.utilities.find_class_by_name import recursive_find_python_class\n",
    "        import nnunetv2\n",
    "        if use_folds is None:\n",
    "            use_folds = nnUNetPredictor.auto_detect_available_folds(model_training_output_dir, checkpoint_name)\n",
    "\n",
    "        dataset_json = load_json(join(model_training_output_dir, 'dataset.json'))\n",
    "        plans = load_json(join(model_training_output_dir, 'plans.json'))\n",
    "        plans_manager = PlansManager(plans)\n",
    "\n",
    "        if isinstance(use_folds, str):\n",
    "            use_folds = [use_folds]\n",
    "\n",
    "        parameters = []\n",
    "        for i, f in enumerate(use_folds):\n",
    "            f = int(f) if f != 'all' else f\n",
    "            checkpoint = torch.load(join(model_training_output_dir, nnunet_checkpoint_name),\n",
    "                                    map_location=torch.device('cpu'))\n",
    "            if i == 0:\n",
    "                trainer_name = checkpoint['trainer_name']\n",
    "                configuration_name = checkpoint['init_args']['configuration']\n",
    "                inference_allowed_mirroring_axes = checkpoint['inference_allowed_mirroring_axes'] if \\\n",
    "                    'inference_allowed_mirroring_axes' in checkpoint.keys() else None\n",
    "\n",
    "            \n",
    "            monai_checkpoint = torch.load(join(model_training_output_dir, monai_checkpoint_name),\n",
    "                                    map_location=torch.device('cpu'))\n",
    "            parameters.append(monai_checkpoint['network_weights'])\n",
    "            \n",
    "        configuration_manager = plans_manager.get_configuration(configuration_name)\n",
    "        # restore network\n",
    "        num_input_channels = determine_num_input_channels(plans_manager, configuration_manager, dataset_json)\n",
    "        trainer_class = recursive_find_python_class(join(nnunetv2.__path__[0], \"training\", \"nnUNetTrainer\"),\n",
    "                                                    trainer_name, 'nnunetv2.training.nnUNetTrainer')\n",
    "        network = trainer_class.build_network_architecture(plans_manager, dataset_json, configuration_manager,\n",
    "                                                           num_input_channels, enable_deep_supervision=False)\n",
    "        predictor.plans_manager = plans_manager\n",
    "        predictor.configuration_manager = configuration_manager\n",
    "        predictor.list_of_parameters = parameters\n",
    "        predictor.network = network\n",
    "        predictor.dataset_json = dataset_json\n",
    "        predictor.trainer_name = trainer_name\n",
    "        predictor.allowed_mirroring_axes = inference_allowed_mirroring_axes\n",
    "        predictor.label_manager = plans_manager.get_label_manager(dataset_json)\n",
    "        if ('nnUNet_compile' in os.environ.keys()) and (os.environ['nnUNet_compile'].lower() in ('true', '1', 't')) \\\n",
    "                and not isinstance(self.network, OptimizedModule):\n",
    "            print('Using torch.compile')\n",
    "            predictor.network = torch.compile(predictor.network)\n",
    "\n",
    "\n",
    "def predict(model_folder, folds, input_folder, output_folder,hive_config_file):\n",
    "    with open(hive_config_file,\"r\") as f:\n",
    "        hive_config_dict = json.load(f)\n",
    "    \n",
    "    os.environ[\"nnUNet_raw\"] = str(Path(hive_config_dict[\"base_folder\"]).joinpath(\"nnUNet_raw_data\"))\n",
    "    os.environ[\"nnUNet_preprocessed\"] = hive_config_dict[\"preprocessing_folder\"]\n",
    "    os.environ[\"nnUNet_results\"] = hive_config_dict[\"results_folder\"]\n",
    "    from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "# instantiate the nnUNetPredictor\n",
    "    predictor = nnUNetPredictor(\n",
    "        tile_step_size=0.5,\n",
    "        use_gaussian=True,\n",
    "        use_mirroring=True,\n",
    "        perform_everything_on_gpu=True,\n",
    "        device=torch.device('cuda', 0),\n",
    "        verbose=False,\n",
    "        verbose_preprocessing=False,\n",
    "        allow_tqdm=True\n",
    "    )\n",
    "    \n",
    "    initialize_from_trained_model_folder(predictor,model_folder,folds,)\n",
    "    \n",
    "    predictor.predict_from_files(input_folder,\n",
    "                                 output_folder,    \n",
    "                             save_probabilities=False, overwrite=False,\n",
    "                                 num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "                                 folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c6f819-7f0e-4a5e-a38d-b46e37d9c519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nnUNetTest/configs/inference.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile nnUNetTest/configs/inference.yaml\n",
    "\n",
    "imports:\n",
    "- $from bundle_scripts.train import get_nnunet_trainer\n",
    "- $from bundle_scripts.train import prepare_nnunet_batch\n",
    "- $import json\n",
    "- $from pathlib import Path\n",
    "- $import os\n",
    "- $from bundle_scripts.inference import predict\n",
    "\n",
    "task_id: 100\n",
    "\n",
    "hive_config_file: \"\"\n",
    "hive_config_dict: \"$json.load(open(@hive_config_file))\"\n",
    "hive_config_file_final: \"$str(Path(os.environ['ROOT_FOLDER']).joinpath(@hive_config_dict['Experiment Name'], @hive_config_dict['Experiment Name'] + '_results','Dataset' + str(@task_id) + '_' + @hive_config_dict['Experiment Name'] + '.json'))\"\n",
    "\n",
    "args:\n",
    "  input_folder: \"@input_folder\"\n",
    "  model_folder: \"@model_folder\"\n",
    "  output_folder: \"@output_folder\"\n",
    "  folds: \"0\"\n",
    "  hive_config_file: \"@hive_config_file_final\"\n",
    "\n",
    "inference:\n",
    "- '$predict(**@args)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408f3300-ed01-4d34-bb50-45c4d6fc848a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-06 12:45:59,955 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2024-01-06 12:45:59,955 - INFO - > config_file: './nnUNetTest/configs/inference.yaml'\n",
      "2024-01-06 12:45:59,956 - INFO - > meta_file: './nnUNetTest/configs/metadata.json'\n",
      "2024-01-06 12:45:59,956 - INFO - > logging_file: './nnUNetTest/configs/logging.conf'\n",
      "2024-01-06 12:45:59,956 - INFO - > run_id: 'inference'\n",
      "2024-01-06 12:45:59,956 - INFO - > bundle_root: './nnUNetTest'\n",
      "2024-01-06 12:45:59,956 - INFO - > task_id: 601\n",
      "2024-01-06 12:45:59,956 - INFO - > hive_config_file: '/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json'\n",
      "2024-01-06 12:45:59,956 - INFO - > model_folder: '/home/jovyan/LungLobeSeg_nnUNet_3D_fullres/results/Dataset102_LungLobeSeg_nnUNet_3D_fullres/nnUNetTrainerHive__nnUNetPlans__3d_fullres'\n",
      "2024-01-06 12:45:59,956 - INFO - > input_folder: '/home/jovyan/MONAI/nnUNetTest/Experiments/LungLobeSeg_Demo_nnUNet_3D_fullres/LungLobeSeg_Demo_nnUNet_3D_fullres_base/nnUNet_raw_data/Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres/imagesTr'\n",
      "2024-01-06 12:45:59,956 - INFO - > output_folder: '/home/jovyan/output'\n",
      "2024-01-06 12:45:59,956 - INFO - ---\n",
      "\n",
      "\n",
      "2024-01-06 12:45:59,957 - INFO - Setting logging properties based on config: ./nnUNetTest/configs/logging.conf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.bundle.workflows ConfigWorkflow.__init__:workflow_type: Current default value of argument `workflow_type=None` has been deprecated since version 1.2. It will be changed to `workflow_type=train` in version 1.4.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 10 cases that I would like to predict\n",
      "overwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 10 cases.\n",
      "Error while terminating subprocess (pid=148): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/__main__.py\", line 31, in <module>\n",
      "    fire.Fire()\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/scripts.py\", line 787, in run\n",
      "    workflow.run()\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/workflows.py\", line 310, in run\n",
      "    return self._run_expr(id=self.run_id)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/workflows.py\", line 344, in _run_expr\n",
      "    return self.parser.get_parsed_content(id, **kwargs) if id in self.parser else None\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/config_parser.py\", line 290, in get_parsed_content\n",
      "    return self.ref_resolver.get_resolved_content(id=id, **kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/reference_resolver.py\", line 193, in get_resolved_content\n",
      "    return self._resolve_one_item(id=id, **kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/reference_resolver.py\", line 163, in _resolve_one_item\n",
      "    self._resolve_one_item(id=d, waiting_list=waiting_list, **kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/reference_resolver.py\", line 175, in _resolve_one_item\n",
      "    item.evaluate(globals={f\"{self._vars}\": self.resolved_content}) if run_eval else item\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/monai/bundle/config_item.py\", line 377, in evaluate\n",
      "    return eval(value[len(self.prefix) :], globals_, locals)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/jovyan/MONAI/nnUNetTest/bundle_scripts/inference_nnunet.py\", line 33, in predict\n",
      "    predictor.predict_from_files(input_folder,\n",
      "  File \"/opt/code/nnunet/nnunetv2/inference/predict_from_raw_data.py\", line 248, in predict_from_files\n",
      "    return self.predict_from_data_iterator(data_iterator, save_probabilities, num_processes_segmentation_export)\n",
      "  File \"/opt/code/nnunet/nnunetv2/inference/predict_from_raw_data.py\", line 341, in predict_from_data_iterator\n",
      "    for preprocessed in data_iterator:\n",
      "  File \"/opt/code/nnunet/nnunetv2/inference/data_iterators.py\", line 112, in preprocessing_iterator_fromfiles\n",
      "    sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUNDLE=\"./nnUNetTest\"\n",
    "\n",
    "export PYTHONPATH=$BUNDLE\n",
    "export ROOT_FOLDER=$BUNDLE\"/Experiments\"\n",
    "\n",
    "python -m monai.bundle run inference\\\n",
    "    --bundle_root \"$BUNDLE\" \\\n",
    "    --task_id \"601\" \\\n",
    "    --hive_config_file \"/home/jovyan/LungLobeSeg_Demo_nnUNet_3D_fullres.json\" \\\n",
    "    --model_folder \"/home/jovyan/LungLobeSeg_nnUNet_3D_fullres/results/Dataset102_LungLobeSeg_nnUNet_3D_fullres/nnUNetTrainerHive__nnUNetPlans__3d_fullres\"\\\n",
    "    --input_folder \"/home/jovyan/MONAI/nnUNetTest/Experiments/LungLobeSeg_Demo_nnUNet_3D_fullres/LungLobeSeg_Demo_nnUNet_3D_fullres_base/nnUNet_raw_data/Dataset601_LungLobeSeg_Demo_nnUNet_3D_fullres/imagesTr\"\\\n",
    "    --output_folder \"/home/jovyan/output\" \\\n",
    "    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n",
    "    --logging_file \"$BUNDLE/configs/logging.conf\" \\\n",
    "    --config_file \"$BUNDLE/configs/inference.yaml\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cc1ec-3404-4ebd-96b7-fe7cc24042dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
